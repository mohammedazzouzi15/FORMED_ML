includes:
  - /media/mohammed/Work/FORMED_ML/config_files/base.yml


model:
  name: hydra
  otf_graph : True
  backbone:
    model: equiformer_v2_backbone
    use_pbc:                  False
    regress_forces:           False
    otf_graph:                True

    enforce_max_neighbors_strictly: False

    max_neighbors:            20
    max_radius:               8.0
    max_num_elements:         100

    num_layers:               8
    sphere_channels:          128
    attn_hidden_channels:     64             # [64, 96] This determines the hidden size of message passing. Do not necessarily use 96.
    num_heads:                8
    attn_alpha_channels:      64              # Not used when `use_s2_act_attn` is True.
    attn_value_channels:      16
    ffn_hidden_channels:      128
    norm_type:                'layer_norm_sh'    # ['rms_norm_sh', 'layer_norm', 'layer_norm_sh']

    lmax_list:                [4]
    mmax_list:                [2]
    grid_resolution:          14              # [18, 16, 14, None] For `None`, simply comment this line.

    num_sphere_samples:       128

    edge_channels:              32
    use_atom_edge_embedding:    True
    distance_function:          'gaussian'
    num_distance_basis:         512           # not used

    attn_activation:          'silu'
    use_s2_act_attn:          False       # [False, True] Switch between attention after S2 activation or the original EquiformerV1 attention.
    ffn_activation:           'silu'      # ['silu', 'swiglu']
    use_gate_act:             False       # [True, False] Switch between gate activation and S2 activation
    use_grid_mlp:             False        # [False, True] If `True`, use projecting to grids and performing MLPs for FFNs.

    alpha_drop:               0.1         # [0.0, 0.1]
    drop_path_rate:           0.1         # [0.0, 0.05]
    proj_drop:                0.0

    weight_init:              'uniform'    # ['uniform', 'normal']
  heads:
    energy:
      module: equiformer_v2_energy_head

optim:
  batch_size:                   4         # 6
  eval_batch_size:              4        # 6
  num_workers: 16
  lr_initial:                   0.0002    # [0.0002, 0.0004], eSCN uses 0.0008 for batch size 96
  
  optimizer: AdamW
  optimizer_params: {"amsgrad": True,weight_decay: 0.0}

  scheduler: "Null"
  max_epochs: 10
  clip_grad_norm: 100
  ema_decay: 0.999

  eval_every: 15000
